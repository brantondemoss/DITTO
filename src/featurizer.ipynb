{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b6a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm, trange\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import wandb\n",
    "from data.wm_dataset import WorldModelDataset\n",
    "from data.common import EquiSampler, transpose_collate\n",
    "from models.world_model import WorldModelRSSM\n",
    "from trainers.metrics import MetricsHelper\n",
    "\n",
    "def transpose_collate(batch):\n",
    "    \"\"\"transposes batch and time dimension\n",
    "    (B, T, ...) -> (T, B, ...)\"\"\"\n",
    "    from torch.utils.data._utils.collate import default_collate\n",
    "    return [torch.transpose(x, 0, 1) for x in default_collate(batch)]\n",
    "\n",
    "\n",
    "class Featurizer(object):\n",
    "    \"\"\" Featurizer reads in conf, loads world model from checkpoint, loads dataset in train dir\n",
    "        calculates features for that dataset under that world model\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_config, conf):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dataset_config = dataset_config\n",
    "        self.conf = conf\n",
    "\n",
    "        # TODO: Don't hardcode this\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        self.batch_size = 500\n",
    "        self.batch_length = 1\n",
    "        \n",
    "        #self.groundtruth = np.load(\"/home/bdemoss/research/div-rl/data/breakout/valf/expertv2-features.npy\")\n",
    "\n",
    "    def get_features(self):\n",
    "        print(\"Building WM Dataset for featurizer...\")\n",
    "        dataloader, dataset = self.build_dataloader()\n",
    "        \n",
    "        chunk_size = math.ceil(len(dataset)/self.batch_size)\n",
    "\n",
    "        print(\"Loading in trained world model...\")\n",
    "        # this is why you need the full conf all the way down here\n",
    "        wm = WorldModelRSSM(self.conf)\n",
    "        wm.load_state_dict(torch.load(self.conf[\"dreamer\"][\"wm_checkpoint\"])['model_state_dict'])\n",
    "        wm.to(self.device)\n",
    "        wm.eval()\n",
    "        wm.requires_grad_(False)\n",
    "\n",
    "        print(\"Getting features...\")\n",
    "        features, actions = self.infer_features(dataset, dataloader, wm)\n",
    "        return features, actions\n",
    "    \n",
    "    def build_dataloader(self):\n",
    "        dataset_config = self.dataset_config.copy()\n",
    "        dataset_config[\"batch_length\"] = self.batch_length\n",
    "        dataset_config[\"rank\"] = self.device\n",
    "        dataset = WorldModelDataset(dataset_config)\n",
    "        batch_sampler = EquiSampler(\n",
    "            len(dataset), self.batch_length, self.batch_size, init_idx = 0)\n",
    "        dataloader = DataLoader(dataset,\n",
    "                                pin_memory=True,\n",
    "                                batch_sampler=batch_sampler,\n",
    "                                collate_fn=transpose_collate)\n",
    "        return dataloader, dataset\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def infer_features(self, dataset, dataloader, wm):\n",
    "        s = time.time()\n",
    "        \n",
    "        # TODO: don't hardcode dim\n",
    "        feats = np.zeros((len(dataset), 2048), dtype=np.float32)\n",
    "        acts = np.zeros((len(dataset), 18), dtype=np.float32)\n",
    "        \n",
    "        for epoch in range(2):\n",
    "            # potentially need to do more than 2 epochs in edge cases when\n",
    "            # episode is longer than chunk_size and extends across 3 chunks\n",
    "            \n",
    "            fake_sampler = iter(EquiSampler(len(dataset), \n",
    "                                            self.batch_length, \n",
    "                                            self.batch_size, \n",
    "                                            init_idx = 0))\n",
    "            if epoch == 0:\n",
    "                in_states = wm.init_state(self.batch_size)\n",
    "            else:\n",
    "                # put the last hidden state of previous chunk\n",
    "                # as the first hidden state of next chunk\n",
    "                # and zero out first one (assume index starts at 0)\n",
    "                in_states = [torch.roll(x, 1, 0) for x in in_states]\n",
    "                in_states[0][0] = torch.zeros_like(in_states[0][0])\n",
    "                in_states[1][0] = torch.zeros_like(in_states[1][0])\n",
    "            for i, batch in enumerate(dataloader):\n",
    "                action, image, reset = [x.to(self.device) for x in batch]\n",
    "\n",
    "                obs = {\"image\": image, \"reset\": reset, \"action\": action}\n",
    "\n",
    "                #with autocast(enabled=True):\n",
    "                features, out_states = wm(obs, in_states)\n",
    "                    # T,B,Feats\n",
    "                    # T = 1 for inference\n",
    "                in_states = out_states\n",
    "\n",
    "                features = features.cpu().numpy().squeeze()\n",
    "                \n",
    "                idxs = np.array(next(fake_sampler))\n",
    "                feats[idxs] = features\n",
    "                acts[idxs] = action.cpu().numpy().squeeze()\n",
    "                if i == 0 or i == 1:\n",
    "                    print(i)\n",
    "                    print(\"first 5 idxs\",idxs[:5])\n",
    "                    print(\"first 5 actions\", action[:5, :5])\n",
    "                    print('NEXT')\n",
    "        print(\"Feature inference took\", int(time.time()-s), \"seconds\")\n",
    "        return feats, acts\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def infer_features2(self, dataset):\n",
    "        \"\"\"Gets the learned state features for this \n",
    "        dataset using whichever WorldModel we loaded.\"\"\"\n",
    "        # size of the final feature array will be (transitions, feature_dim)\n",
    "        s = time.time()\n",
    "        fake_sampler = iter(EquiSampler(len(self.dataset), \n",
    "                                            self.batch_length, \n",
    "                                            self.batch_size, \n",
    "                                            init_idx = 0))\n",
    "        feats = np.zeros((len(self.dataset), 2048), dtype=np.float32)\n",
    "        \n",
    "        in_states = self.wm.init_state(self.batch_size)\n",
    "        \n",
    "        chunk_size = self.chunk_size\n",
    "        \n",
    "        print('in infer_features2')\n",
    "        for i, batch in enumerate(self.dataloader):\n",
    "            action, image, reset = [x.to(self.device) for x in batch]\n",
    "\n",
    "            obs = {\"image\": image, \"reset\": reset, \"action\": action}\n",
    "\n",
    "            with autocast(enabled=True):\n",
    "                features, out_states = self.wm(obs, in_states)\n",
    "                # T,B,Feats\n",
    "            in_states = out_states\n",
    "            \n",
    "            features = features.cpu().numpy().squeeze()\n",
    "            \n",
    "            feats[np.array(next(fake_sampler))] = features\n",
    "        print(\"Feature2 inference took\", int(time.time()-s), \"seconds\")\n",
    "        return feats\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def infer_features_exact(self, dataset, dataloader, wm):\n",
    "        \"\"\"Gets the learned state features for this \n",
    "        dataset using whichever WorldModel we loaded.\"\"\"\n",
    "        \n",
    "        # size of the final feature array will be (transitions, feature_dim)\n",
    "        print(\"Begin inference\")\n",
    "        s = time.time()\n",
    "        feature_list = []\n",
    "        in_states = wm.init_state(1)\n",
    "        for i in range(len(dataset)):\n",
    "            action, image, reset = [x.to(self.device) for x in dataset.get_trans(i)]\n",
    "            obs = {\"image\": image, \"reset\": reset, \"action\": action}\n",
    "\n",
    "            with autocast(enabled=False):\n",
    "                features, out_states = wm(obs, in_states)\n",
    "            feature_list.append(features.cpu().numpy().squeeze())\n",
    "            in_states = out_states\n",
    "            if i%10000 == 0:\n",
    "                print(\"inferred\", i, \"features in\", int(time.time()-s), \"seconds\")\n",
    "\n",
    "        print(\"got\", len(feature_list), \"features\")\n",
    "\n",
    "        print(\"Feature inference took\", int(time.time()-s), \"seconds\")\n",
    "        features = np.stack(feature_list)\n",
    "        return features\n",
    "\n",
    "\n",
    "def load_conf(config_file, env_name):\n",
    "    with open(config_file, \"r\") as f:\n",
    "        raw_conf = yaml.safe_load(f)\n",
    "    base_conf = raw_conf[\"base\"]\n",
    "    data_conf = raw_conf[\"data\"]\n",
    "    env_conf = raw_conf[env_name]\n",
    "    conf = {**base_conf, **env_conf}\n",
    "    if \"dreamer\" in raw_conf:\n",
    "        dreamer_conf = raw_conf[\"dreamer\"]\n",
    "        conf[\"dreamer\"] = dreamer_conf\n",
    "    conf[\"data\"] = data_conf\n",
    "    return conf\n",
    "\n",
    "\n",
    "def run_trainer(conf):\n",
    "    trainer = Featurizer(conf[\"data\"][\"train\"], conf)\n",
    "    #wandb.watch(trainer.model)\n",
    "    #trainer.train()\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def main():\n",
    "    config_file = \"../config/config_agent.yaml\"\n",
    "    env_name = \"atari_breakout\"\n",
    "    conf = load_conf(config_file, env_name)\n",
    "\n",
    "    #wandb.login()\n",
    "    #wandb.init(project=\"world-model\")\n",
    "    trainer = run_trainer(conf)\n",
    "    # launch_ddp_train(conf)\n",
    "    return trainer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f29cdb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building WM Dataset for featurizer...\n",
      "Building WorldModelDataset for ['breakout-expert-v2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post fix in fix_actions [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "total transitions: 1000000\n",
      "num batch elements: 1000000\n",
      "Chunk size: 2000\n",
      "n steps: 2000\n",
      "Loading in trained world model...\n",
      "Getting features...\n",
      "Chunk size: 2000\n",
      "n steps: 2000\n",
      "iters[:5] [0, 2000, 4000, 6000, 8000]\n",
      "iters[:5] [0, 2000, 4000, 6000, 8000]\n",
      "0\n",
      "first 5 idxs [   0 2000 4000 6000 8000]\n",
      "first 5 actions tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.]]], device='cuda:0')\n",
      "NEXT\n",
      "iters[:5] [1, 2001, 4001, 6001, 8001]\n",
      "iters[:5] [1, 2001, 4001, 6001, 8001]\n",
      "1\n",
      "first 5 idxs [   1 2001 4001 6001 8001]\n",
      "first 5 actions tensor([[[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.]]], device='cuda:0')\n",
      "NEXT\n",
      "Chunk size: 2000\n",
      "n steps: 2000\n",
      "iters[:5] [0, 2000, 4000, 6000, 8000]\n",
      "iters[:5] [0, 2000, 4000, 6000, 8000]\n",
      "0\n",
      "first 5 idxs [   0 2000 4000 6000 8000]\n",
      "first 5 actions tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.]]], device='cuda:0')\n",
      "NEXT\n",
      "iters[:5] [1, 2001, 4001, 6001, 8001]\n",
      "iters[:5] [1, 2001, 4001, 6001, 8001]\n",
      "1\n",
      "first 5 idxs [   1 2001 4001 6001 8001]\n",
      "first 5 actions tensor([[[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.]]], device='cuda:0')\n",
      "NEXT\n",
      "Feature inference took 29 seconds\n"
     ]
    }
   ],
   "source": [
    "trainer = main()\n",
    "features, actions = trainer.get_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04d2ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape, actions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e41fe2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = np.load(\"/home/bdemoss/research/div-rl/data/breakout/valf/expertv2-features.npy\")\n",
    "gta = np.load(\"/home/bdemoss/research/div-rl/data/breakout/valfeatures-expertv2-autocast.npy\").squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95240d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WorldModelRSSM(\n",
       "  (encoder): EncoderModel(\n",
       "    (encoder): CnnEncoder(\n",
       "      (model): Sequential(\n",
       "        (0): Conv2d(1, 48, kernel_size=(4, 4), stride=(2, 2))\n",
       "        (1): ELU(alpha=1.0)\n",
       "        (2): Conv2d(48, 96, kernel_size=(4, 4), stride=(2, 2))\n",
       "        (3): ELU(alpha=1.0)\n",
       "        (4): Conv2d(96, 192, kernel_size=(4, 4), stride=(2, 2))\n",
       "        (5): ELU(alpha=1.0)\n",
       "        (6): Conv2d(192, 384, kernel_size=(4, 4), stride=(2, 2))\n",
       "        (7): ELU(alpha=1.0)\n",
       "        (8): Flatten(start_dim=1, end_dim=-1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (rssm_core): RSSMCore(\n",
       "    (cell): RSSMCell(\n",
       "      (z_mlp): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "      (a_mlp): Linear(in_features=18, out_features=1000, bias=False)\n",
       "      (in_norm): LayerNorm((1000,), eps=0.001, elementwise_affine=True)\n",
       "      (gru): GRUCellStack(\n",
       "        (layers): ModuleList(\n",
       "          (0): GRUCell(1000, 1024)\n",
       "        )\n",
       "      )\n",
       "      (prior_mlp_h): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "      (prior_norm): LayerNorm((1000,), eps=0.001, elementwise_affine=True)\n",
       "      (prior_mlp): Linear(in_features=1000, out_features=1024, bias=True)\n",
       "      (post_mlp_h): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "      (post_mlp_e): Linear(in_features=1536, out_features=1000, bias=False)\n",
       "      (post_norm): LayerNorm((1000,), eps=0.001, elementwise_affine=True)\n",
       "      (post_mlp): Linear(in_features=1000, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): DecoderModel(\n",
       "    (decoder): ConvDecoder(\n",
       "      (model): Sequential(\n",
       "        (0): Flatten(start_dim=0, end_dim=1)\n",
       "        (1): Linear(in_features=2048, out_features=1536, bias=True)\n",
       "        (2): Unflatten(dim=-1, unflattened_size=(1536, 1, 1))\n",
       "        (3): ConvTranspose2d(1536, 192, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (4): ELU(alpha=1.0)\n",
       "        (5): ConvTranspose2d(192, 96, kernel_size=(5, 5), stride=(2, 2))\n",
       "        (6): ELU(alpha=1.0)\n",
       "        (7): ConvTranspose2d(96, 48, kernel_size=(6, 6), stride=(2, 2))\n",
       "        (8): ELU(alpha=1.0)\n",
       "        (9): ConvTranspose2d(48, 1, kernel_size=(6, 6), stride=(2, 2))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "config_file = \"../config/config_agent.yaml\"\n",
    "env_name = \"atari_breakout\"\n",
    "conf = load_conf(config_file, env_name)\n",
    "wm = WorldModelRSSM(conf)\n",
    "wm.load_state_dict(torch.load(conf[\"dreamer\"][\"wm_checkpoint\"])['model_state_dict'])\n",
    "wm.to(device)\n",
    "wm.eval()\n",
    "wm.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b78648a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_diff = np.sum(np.square(gta[:,:1024] - features[:,:1024]))/1000000.\n",
    "print(total_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5f92099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.load(\"/home/bdemoss/research/div-rl/data/breakout/valf/breakout-expert-v2.npz\")\n",
    "print(b[\"action\"].shape)\n",
    "print(actions.shape)\n",
    "np.any(b[\"action\"] == actions)\n",
    "i = 1\n",
    "np.all(b[\"action\"][i] == actions[i])\n",
    "actions[:5]\n",
    "b[\"action\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0480ac35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import d4rl_atari\n",
    "import gym\n",
    "env = gym.make('breakout-expert-v2')\n",
    "data = env.get_dataset()\n",
    "data[\"actions\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c14dd0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 0, 0, 0, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"actions\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "209abd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: tensor([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.]]], device='cuda:0') torch.Size([1, 1, 18])\n",
      "waction: tensor([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.]]], device='cuda:0') torch.Size([1, 1, 18])\n",
      "differences (0.0, 18.0)\n",
      "(2048,)\n",
      "[torch.Size([1, 1024]), torch.Size([1, 1024])]\n"
     ]
    }
   ],
   "source": [
    "def cdiff(x,y):\n",
    "    x=x.squeeze()\n",
    "    y=y.squeeze()\n",
    "    randiff = np.sum(np.square(x[1024:]-y[1024:]))\n",
    "    detdiff = np.sum(np.square(x[:1024]-y[:1024]))\n",
    "    return detdiff, randiff\n",
    "\n",
    "index = 666\n",
    "in_states = torch.tensor(trainer.ampfeatures[index]).unsqueeze(0).to(trainer.device)\n",
    "in_states = (in_states[..., :1024], in_states[..., 1024:])\n",
    "action, image, reset = [x.to(trainer.device) for x in trainer.dataset.get_trans(index)]\n",
    "\n",
    "print('action:',action,action.shape)\n",
    "waction = torch.tensor([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "          0.]]]).to(trainer.device)\n",
    "print('waction:',waction,waction.shape)\n",
    "\n",
    "obs = {\"image\": image, \"reset\": reset, \"action\": action}\n",
    "wobs = {\"image\": image, \"reset\": reset, \"action\": waction}\n",
    "\n",
    "with autocast(enabled=True):\n",
    "    features, out_states = trainer.wm(obs, in_states)\n",
    "with autocast(enabled=True):\n",
    "    wfeatures, wout_states = trainer.wm(wobs, in_states)\n",
    "    \n",
    "features = features.detach().cpu().numpy().squeeze()\n",
    "wfeatures = wfeatures.detach().cpu().numpy().squeeze()\n",
    "\n",
    "print('differences',cdiff(features,wfeatures))\n",
    "\n",
    "print(features.shape)\n",
    "print([out_states[x].shape for x in range(2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff8f744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size: 10000\n",
      "n steps: 10000\n",
      "Chunk size: 10000\n",
      "n steps: 10000\n",
      "10000\n",
      "10000\n",
      "[0, 10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000, 110000, 120000, 130000, 140000, 150000, 160000, 170000, 180000, 190000, 200000, 210000, 220000, 230000, 240000, 250000, 260000, 270000, 280000, 290000, 300000, 310000, 320000, 330000, 340000, 350000, 360000, 370000, 380000, 390000, 400000, 410000, 420000, 430000, 440000, 450000, 460000, 470000, 480000, 490000, 500000, 510000, 520000, 530000, 540000, 550000, 560000, 570000, 580000, 590000, 600000, 610000, 620000, 630000, 640000, 650000, 660000, 670000, 680000, 690000, 700000, 710000, 720000, 730000, 740000, 750000, 760000, 770000, 780000, 790000, 800000, 810000, 820000, 830000, 840000, 850000, 860000, 870000, 880000, 890000, 900000, 910000, 920000, 930000, 940000, 950000, 960000, 970000, 980000, 990000]\n",
      "[0, 10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000, 110000, 120000, 130000, 140000, 150000, 160000, 170000, 180000, 190000, 200000, 210000, 220000, 230000, 240000, 250000, 260000, 270000, 280000, 290000, 300000, 310000, 320000, 330000, 340000, 350000, 360000, 370000, 380000, 390000, 400000, 410000, 420000, 430000, 440000, 450000, 460000, 470000, 480000, 490000, 500000, 510000, 520000, 530000, 540000, 550000, 560000, 570000, 580000, 590000, 600000, 610000, 620000, 630000, 640000, 650000, 660000, 670000, 680000, 690000, 700000, 710000, 720000, 730000, 740000, 750000, 760000, 770000, 780000, 790000, 800000, 810000, 820000, 830000, 840000, 850000, 860000, 870000, 880000, 890000, 900000, 910000, 920000, 930000, 940000, 950000, 960000, 970000, 980000, 990000]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from data.common import EquiSampler\n",
    "sampler = EquiSampler(1000000, 1, 100, init_idx = 0)\n",
    "sampler2 = EquiSampler(1000000, 1, 100, init_idx = 0)\n",
    "batch_sampler = iter(sampler)\n",
    "batch_sampler2 = iter(sampler2)\n",
    "print(len(sampler))\n",
    "print(len(sampler2))\n",
    "\n",
    "idxs = []\n",
    "idxs2 = []\n",
    "\n",
    "while True:\n",
    "    n = next(batch_sampler, None)\n",
    "    nn = next(batch_sampler2, None)\n",
    "    if n is None:\n",
    "        break\n",
    "    idxs.append(n)\n",
    "    idxs2.append(nn)\n",
    "\n",
    "print(idxs[0])\n",
    "\n",
    "\n",
    "print(idxs2[0])\n",
    "\n",
    "print(idxs2==idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c46d40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
